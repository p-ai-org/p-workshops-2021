{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31bf597",
   "metadata": {},
   "source": [
    "# P-ai AI/ML Workshop: Session 2\n",
    "\n",
    "Welcome to P-ai's second session of the AI/ML workshop series! Today we'll learn about\n",
    "- Supervised learning, unsupervised learning, and reinforcement learning\n",
    "- Supervised learning in more detail\n",
    "- Linear regression and logistic regression\n",
    "\n",
    "And we'll finish off with a case study that will allow us to apply what we've learned.\n",
    "<img src=\"https://images.squarespace-cdn.com/content/5d5aca05ce74150001a5af3e/1580018583262-NKE94RECI46GRULKS152/Screen+Shot+2019-12-05+at+11.18.53+AM.png?content-type=image%2Fpng\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0eec4",
   "metadata": {},
   "source": [
    "## 0. Session 1 Review\n",
    "Here are some key points from last week's session:\n",
    "- Machine learning is a tool used for finding finding patterns in data, typically in order to make predictions or perform a task without being explicitly programmed\n",
    "- Often, a machine learning algorithm requires finding a set of *parameters* that optimizes some metric (like minimizing error)\n",
    "- Applying machine learning requires many steps of data collection, processing, training, and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2889b48",
   "metadata": {},
   "source": [
    "## 1. Domains of Machine Learning\n",
    "Let's outline three of the biggest \"domains\" of machine learning tasks we should be aware of:\n",
    "\n",
    "<img src=\"https://assets.wordstream.com/s3fs-public/styles/simple_image/public/images/machine-learning1.png?SnePeroHk5B9yZaLY7peFkULrfW8Gtaf&itok=yjEJbEKD\" width=\"600px\">\n",
    "\n",
    "Let's break down the three biggest branches: **supervised learning**, **unsupervised learning**, and **reinforcement learning**.\n",
    "\n",
    "### Supervised learning\n",
    "Supervised learning requires **labeled data**, that is, the training data consists of inputs and known correct outputs. This is the \"bread and butter\" kind of machine learning you will most likely come across. Here are a few examples:\n",
    "- Given a dataset of flower measurements and their corresponding species, learn to classify a flower's species from its measurements\n",
    "- Given a dataset of 32x32 images of handwritten digits, learn to classify an image as one of ten digits\n",
    "- Given a dataset of house pictures and their prices, learn to predict the price of a house given a picture\n",
    "\n",
    "In all these cases, we have lots of examples of the \"correct answers\", and need a machine learning model to learn what function maps from input to output. More on this later!\n",
    "\n",
    "### Unsupervised learning\n",
    "Unsupervised learning deals with **unlabeled data**, and the two most common tasks in the unsupervised learning space is clustering and dimensionality reduction. We'll cover these in more detail later, but to give an idea of what UL is about, let's consider clustering. As the name might suggest, the task of clustering involves taking a bunch of data points and grouping them together into clusters. We don't need labels to perform this!\n",
    "\n",
    "For example, let's say you had a database of songs with numerical data like BPM, danceability, loudness, valence, etc. (these are <a href=\"https://developer.spotify.com/documentation/web-api/reference/#object-audiofeaturesobject\">actual metrics you can get from the Spotify API!</a>). You might be interested to see how songs cluster together, and whether those clusters match our perception of genre.\n",
    "\n",
    "### Reinforcement Learning (RL)\n",
    "Reinforcement learning is a little different from supervised and unsupervised learning. RL involves training an **agent** to **interact with an environment** to maximize some reward. In other words, the agent can perform actions which change the environment and yield some reward or punishment. In theory, this is a simplified model of how humans learn (think touching a hot stove, or being rewarded for good grades). The most classic examples of RL are models that play games like Go (see <a href=\"https://deepmind.com/research/case-studies/alphago-the-story-so-far\">AlphaGo</a>) or Mario. RL can be more nuanced, though, like stock trading or learning language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d75d9",
   "metadata": {},
   "source": [
    "## 2. Supervised Learning\n",
    "\n",
    "Supervised learning is a great place to start with machine learning because it explains most of the most fundamental concepts behind ML, is very applicable, and allows us to start simple. Before going further, let's break down the two big subdomains of supervised learning:\n",
    "\n",
    "<img src=\"https://lh4.googleusercontent.com/K17BRCQTR5hHU-qOthrs9KIQa4DLAWJh5jeXkyn6NZRQfimHnCAadWbw3EaZPZl1bit2IBQPeBv1CZURiyFYkIDPH1Z3Pb0O_qkeS9av7vrEtQLpMLWdtDJ7YNlRki8CoAsY8bmn\" width=\"450px\">\n",
    "\n",
    "- **Regression** is about predicting a **quantity**; in the example above, temperature. Another example might be predicting someone's age given a picture of their face.\n",
    "- **Classification** is about assigning a **class** (or label) to a given data point; in the example above, \"hot\" or \"cold\". Another example might be classifying paintings as Da Vinci, Van Gogh, or Dali.\n",
    "\n",
    "Recall how we said supervised learning requires **labeled data**. Let's call the input data $X$ and the output data (what we're trying to predict) $y$. In the most gross oversimplification of machine learning, supervised learning is about finding a function $f$ such that:\n",
    "\n",
    "$$f(X) \\approx y$$\n",
    "\n",
    "That may sound obvious and/or unhelpful, but just keep it in mind as we continue through today's session, and especially when we cover deep learning; we are *using an algorithm to find a mapping from input to output*! In the words of my first year calculus professor:\n",
    "\n",
    "> \"It's not magic; it's just linear algebra.\"  \n",
    "> – Prof Karp\n",
    "\n",
    "Okay, let's start getting into the weeds a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f250ea",
   "metadata": {},
   "source": [
    "## 3. Linear Regression and Logistic Regression\n",
    "\n",
    "- **Q.** *Why?*  \n",
    "- **A.** These two algorithms are relatively simple but teach a lot about core machine learning concepts, and also serve as the foundation for deep learning.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "If you attended last week's session or looked over the notebook, you'll remember that we quickly covered linear regression. But this time, it's for real. Recall that for linear regression in 2D, we try to fit a line to the data. That means, if we have an input variable $x$ and an output variable $y$, we wanted to find parameters $m$ and $b$ so we could get:\n",
    "\n",
    "$$f(x) = mx + b \\approx y$$\n",
    "\n",
    "So we have something to look at, let's generate some fake data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages we'll need later\n",
    "!pip install pandas seaborn scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helpful modules\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b321aa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn has a function for generating roughly linear data\n",
    "bias = 20\n",
    "X, y, coef = datasets.make_regression(n_features=1, noise=10, bias=bias, coef=True, random_state=43)\n",
    "# don't worry about this\n",
    "x = [x_[0] for x_ in X]\n",
    "sns.scatterplot(x=x, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ad91f",
   "metadata": {},
   "source": [
    "If you want to make this more concrete, imagine that the $x$ axis is the average salary in a particular city, and the $y$ axis is the average cost of living for that city. Your boss hears you \"know machine learning\", so they ask you to \"use AI\" to predict the cost of living for a city given the average salary. Since we can see the relationship is linear, and we're trying to predict a **value** (regression, not classification), linear regression is a good choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be3614",
   "metadata": {},
   "source": [
    "### An ML Spin on Linear Regression\n",
    "If you've ever taken a stats course, you're probably falling asleep at this point. Line of best fit might not seem like machine learning, but you'll see in a second that it is- it's just arguably the simplest instance of it.\n",
    "\n",
    "Imagine you're the machine, and you want to learn how to model that relationship between salary and cost of living. What do you need to figure out? Recall the formula:\n",
    "$$\n",
    "\\hat{y}=wx+b\n",
    "$$\n",
    "Don't mind the change from slope variable $m$ to $w$. We'll start to slide over to ML lingo, where we call the slope **weight**. This makes sense, because when we multiply a variable by an amount, we're basically assigning a *weight* to that variable. Also, instead of calling $b$ the y-intercept, we call it the **bias**. So in this case, we, the machine, have to figure out what $w$ and $b$ are from the data alone.\n",
    "\n",
    "> Where do we start?\n",
    "\n",
    "How about a random guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineLinearModel(w, b):\n",
    "    ''' Note that defineModel returns a function! '''\n",
    "    def model(x):\n",
    "        return w*x + b\n",
    "    return model\n",
    "    \n",
    "# Define our model with w (weight) = 2 and b (bias) = 10\n",
    "model = defineLinearModel(2, 10)\n",
    "# Here's our prediction for x = 5\n",
    "model(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b6fa2",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the $x$ data in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [model(x_) for x_ in x]\n",
    "# Print first 10\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2d69a",
   "metadata": {},
   "source": [
    "Let's compare our predictions with the actual y values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d141554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just putting our variables into a table for readability\n",
    "df = pd.DataFrame(columns=[\"x\", \"y_pred\", \"y\"], \n",
    "                  data={\n",
    "                      \"x\": x, \n",
    "                      \"y_pred\": y_pred, \n",
    "                      \"y\": y\n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7508ac3",
   "metadata": {},
   "source": [
    "> What now?\n",
    "\n",
    "We guessed our parameters and made predictions for all our $x$ values. We need to know **how bad** our predictions were compared to the actual known $y$ values.\n",
    "\n",
    "> How should we compare them?\n",
    "\n",
    "One simple idea might be to take the absolute difference between the two, and call it the error. Here's what that would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbaebf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"error\"] = (df[\"y_pred\"] - df[\"y\"]).apply(abs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4117554e",
   "metadata": {},
   "source": [
    "We could then average together all these errors to distill our predictions down to one number that represents the \"badness\" of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c363344",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"error\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c10a7f",
   "metadata": {},
   "source": [
    "This works, but a more common choice for regression is **mean squared error**, or **MSE**. All it does is take the square of the difference between predicted and actual values, and then takes the mean across all predictions. Also, to slip into some more machine learning terminology, we'll call the resulting value **loss** instead of error, and the function that produces loss (in this case, MSE) our **loss function**. If you're interested, here's the formal definition of MSE:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}{(\\hat{y}_i-y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb917bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ML lingo\n",
    "df = df.rename(columns={\"error\": \"loss\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdaeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the difference, then square\n",
    "df[\"loss\"] = (df[\"y_pred\"] - df[\"y\"]).map(lambda x: x**2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"loss\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64622269",
   "metadata": {},
   "source": [
    "There's our loss! Let's try a different set of parameters and see if we increase or decrease the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New model: w = 10 and b = 15\n",
    "model_2 = defineLinearModel(10, 15)\n",
    "# Make predictions (inference)\n",
    "df[\"y_pred\"] = [model_2(x_) for x_ in df[\"x\"]]\n",
    "# Calculate loss (MSE)\n",
    "df[\"loss\"] = (df[\"y_pred\"] - df[\"y\"]).map(lambda x: x**2)\n",
    "df[\"loss\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb271e",
   "metadata": {},
   "source": [
    "Nice! By tweaking our model's parameters, we can decrease the loss.\n",
    "\n",
    "> So what?\n",
    "\n",
    "Remember from last week's session that machine learning is built on the idea that *learning* is roughly equivalent to *minimizing some cost* (or conversely, increasing some reward). This makes intuitive sense, because the closer the loss gets to 0, the closer the model's predictions are to the true values, so we would say the model has \"learned\" the data. That means that the machine learning model's job is to **find the parameters that minimize the loss**.\n",
    "\n",
    "> How do machine learning models find those optimal parameters?\n",
    "\n",
    "That is a very important question with an equally important question, but we'll have to save for next session in the interest of time :) For now, just know that there's a process that many machine learning algorithms use to find those parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fbe000",
   "metadata": {},
   "source": [
    "Let's use `scikit-learn`'s <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">LinearRegression</a> model to fit our generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to remember what our data looks like\n",
    "for i in range(10):\n",
    "    print(f\"{X[i]} => {y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b1cea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d52defe",
   "metadata": {},
   "source": [
    "Yep, that's right. All it took was to call `.fit()`. When we called `.fit()`, the model followed the linear regression algorithm to determine which parameters (namely, one weight and a bias) minimized the loss. Just to prove I'm not pulling this out of thin air, check out this line taken directly from the documentation (linked above):\n",
    "> LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Remember, learning (in ML) is nearly always about optimizing some value– in this case, sum of squares (almost the same as mean squared error, the difference is just a factor of $n$).\n",
    "\n",
    "Let's have a look at which parameters the model settled on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The calculated weight is {reg.coef_[0]}\")\n",
    "print(f\"The calculated bias is {reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25207800",
   "metadata": {},
   "source": [
    "And to compare these values to the \"correct\" ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The 'actual' weight is {coef}\")\n",
    "print(f\"The 'actual' bias is {bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c87e2be",
   "metadata": {},
   "source": [
    "Very close! Since the generated data has some noise associated with it, this is likely as close as we could possibly get. Let's run some inference and check that our predictions are close:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1610537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (inference)\n",
    "df[\"y_pred\"] = reg.predict(X)\n",
    "# Calculate loss (MSE)\n",
    "df[\"loss\"] = (df[\"y_pred\"] - df[\"y\"]).map(lambda x: x**2)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a2d81",
   "metadata": {},
   "source": [
    "And, let's plot the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(slope, intercept):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = intercept + slope * x_vals\n",
    "    plt.plot(x_vals, y_vals, '--', color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337dd4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x, y=y)\n",
    "abline(reg.coef_[0], reg.intercept_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2b5d07",
   "metadata": {},
   "source": [
    "Looks right!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fdb18",
   "metadata": {},
   "source": [
    "### When is Linear Regression Not a Good Idea?\n",
    "\n",
    "This one won't come as much of a surprise- don't use linear regression is your data isn't roughly linear! Let's see what happens if you try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate quadratic data\n",
    "def quadratic(x):\n",
    "    return 2 + 3 * (x - 4)**2\n",
    "\n",
    "X_quad = np.arange(-20, 20)\n",
    "y_quad = [quadratic(x) for x in X_quad]\n",
    "\n",
    "sns.scatterplot(x=X_quad, y=y_quad)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model\n",
    "reg_quad = LinearRegression().fit(np.expand_dims(X_quad, axis=-1), y_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check\n",
    "sns.scatterplot(x=X_quad, y=y_quad)\n",
    "abline(reg_quad.coef_[0], reg_quad.intercept_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427f64",
   "metadata": {},
   "source": [
    "Sorry, not even close. But that was literally the best it could do!\n",
    "\n",
    "This gets at a larger point about machine learning. Soon, you'll know about a dozen different machine learning algorithms, which is awesome! But you need to also remember the strengths and weaknesses of each one. For example, linear regression model will never truly learn non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221470fc",
   "metadata": {},
   "source": [
    "### Generalizing!\n",
    "\n",
    "We're about to do something pretty cool. Just by reframing how we think of a linear regression model, we'll find ourselves at the doorstep of deep learning! First, let's visualize a linear regression model like this:\n",
    "<img src=\"images/lin_reg_1d.png\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b747fd",
   "metadata": {},
   "source": [
    "We have one input variable, $x$, and one output variable $y$. We multiply $x$ by its weight $w$, and add the bias $b$ to get our output variable. Simple enough, right? What if we had multiple input variables? What would that look like?\n",
    "Mathematically, we would be going from\n",
    "\n",
    "$$\n",
    "wx + b = y\n",
    "$$\n",
    "to\n",
    "$$\n",
    "w_1x_1 + w_2x_2 + ... + w_nx_n + b = y\n",
    "$$\n",
    "\n",
    "If you think about that, it should make sense. Each variable $x_i$ deserves their own weight (how much, and in which direction does that variable affect the output variable $y$?), and we still need a bias that doesn't depend on any of the inputs. Here's how we can visualize it:\n",
    "<img src=\"images/lin_reg_nd.png\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d10fa6",
   "metadata": {},
   "source": [
    "Just to show you how this could be done with code, let's use the same `scikit-learn` data generator function to generate 3D input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7462f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D input => scalar output\n",
    "X_2, y_2 = datasets.make_regression(n_features=3, noise=10, bias=20, random_state=43)\n",
    "# Print first 10\n",
    "for i in range(10):\n",
    "    print(f\"{X_2[i]} => {y_2[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aede17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like before, we can fit a linear regression model to this data\n",
    "reg_2 = LinearRegression().fit(X_2, y_2)\n",
    "print(f\"Weights: {reg_2.coef_}\")\n",
    "print(f\"Bias: {reg_2.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ad9f8",
   "metadata": {},
   "source": [
    "Notice how there's **three** weights; one for each input variable! Like always, we can make predictions using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91060283",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = np.array([[-0.5, 1, 0.25]])\n",
    "print(f\"Input X: {input_x}\")\n",
    "pred = reg_2.predict(input_x)\n",
    "print(f\"Prediction: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a6165",
   "metadata": {},
   "source": [
    "Cool! Let's move on to logistic regression..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da111b45",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The first thing to know about logistic regression is that it's **used for classification, not regression tasks**. This probably seems confusing given \"regression\" is literally in the name, but we can explain that in a second. Let's take a quick second to remember what the difference between regression and classification is:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Yves-Matanga/publication/326175998/figure/fig9/AS:644582983352328@1530691967314/Classification-vs-Regression.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b5a8ef",
   "metadata": {},
   "source": [
    "While regression tries to fit a line or curve to the data, classification seeks to *label* or *classify* data points as belonging to one class or another. Whenever you're performing classification, you're either directly or indirectly creating a **decision boundary**, which separates one class from another (that's what's being illustrated above).\n",
    "\n",
    "It's also worth noting that classification is usually further broken down into a few types:\n",
    "- **Binary classification**: there are only two possible classes\n",
    "    - e.g. Identifying tumors from brain scans; 0 = no tumor detected, 1 = tumor detected\n",
    "- **Multiclass classification**: there are three or more possible classes\n",
    "    - e.g. Identifying dog breeds from pictures; could have dozens of classes\n",
    "- **Multilabel classification**: there are multiple, non-mutually exclusive labels that can be assigned\n",
    "    - e.g. Identifying any number of objects in a photo\n",
    "    \n",
    "So for binary classification, we only need to predict a 0 or a 1. How does that happen? Let's take a look at the formula:\n",
    "\n",
    "$$\n",
    "x_1w_1 + x_2w_2 + ... + x_nw_n + b = ln\\left( \\frac{\\hat{p}}{1-\\hat{p}} \\right)\n",
    "$$\n",
    "\n",
    "The entire left side should look familiar- it's exactly the same as linear regression! That is, it's a linear combination of all the input variables, plus a constant bias. The key difference is that we're not trying to predict any old value; we want to predict a *probability*. Before going further, let's rearrange the terms to solve for that predicted probability $\\hat{p}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{let}\\hspace{0.5em}x_1w_1 + x_2w_2 + ... + x_nw_n + b &= X\n",
    "    \\\\\n",
    "    X &= ln\\left( \\frac{\\hat{p}}{1-\\hat{p}} \\right)\n",
    "    \\\\\n",
    "    e^X &= \\left( \\frac{\\hat{p}}{1-\\hat{p}} \\right)\n",
    "    \\\\\n",
    "    e^X (1-\\hat{p}) &= \\hat{p}\n",
    "    \\\\\n",
    "    e^X - e^X\\hat{p} &= \\hat{p}\n",
    "    \\\\\n",
    "    e^X &= \\hat{p} + e^X\\hat{p}\n",
    "    \\\\\n",
    "    e^X &= \\hat{p} (1 + e^X)\n",
    "    \\\\\n",
    "    \\hat{p} = \\frac{e^X}{1 + e^X}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Or, equivalently (multiply top and bottom by $e^{-X}$):\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{1 + e^{-X}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b67814",
   "metadata": {},
   "source": [
    "Okay, now we're ready to interpret these formulas. You may not have known, but there are two very special functions we came across just now:\n",
    "\n",
    "$$\n",
    "logit(p) = ln\\left( \\frac{p}{1-p} \\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The logit function takes in a probability (from 0 to 1) and returns a real number from negative infinity to positive infinity (called a logit). The sigmoid function is the inverse of the logit function, so it takes in a logit and returns a probability from 0 to 1. Let's visualize the sigmoid function:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*JHWL_71qml0kP_Imyx4zBg.png\" width=\"600px\">\n",
    "\n",
    "Basically, for very large negative inputs, the sigmoid function returns nearly 0. For very large positive inputs, it returns nearly 1. As $x$ approaches 0, $sigmoid(x) \\rightarrow \\frac{1}{2}$. Let's look at the formula again and see if this makes sense:\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{1 + e^{-X}} = sigmoid(x_1w_1 + x_2w_2 + ... + x_nw_n + b)\n",
    "$$\n",
    "\n",
    "See what's happening? First, we perform all the usual linear regression stuff (multiply each input variable by its weight, add the bias), but then we transform that real number into a probability by passing it through the sigmoid function! That probability represents the probability that the input belongs to the positive class. Let's make this a little more visual by returning to the idea of a *decision boundary*:\n",
    "\n",
    "<img src=\"https://scipython.com/static/media/uploads/blog/logistic_regression/decision-boundary.png\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c644ee7",
   "metadata": {},
   "source": [
    "It looks like someone has already fitted a logistic regression model to this dataset to separate the blue points from the orange points. Take a look at that decision boundary- what does a decision boundary represent? It means that all the points on one side of the line will be classified one way, and the points on the other side will be classified the other. Let's say that blue represents the negative class (0) and orange represents the positive class (1). The further you go above the line, the higher the predicted probability of belonging to the positive class ($\\hat{p} \\rightarrow 1$). The further below the line you go, the lower the predicted probability of belonging to the positive class ($\\hat{p} \\rightarrow 0$). Right on the line, you can't make a decision; that corresponds to $\\hat{p} = \\frac{1}{2}$. See the connection?\n",
    "\n",
    "Now consider what it means to be above the line in this case. If the decision boundary is $x_1w_1 + x_2w_2 + b = 0$, then the orange region is represented by $x_1w_1 + x_2w_2 + b > 0$ and the blue region is represented by $x_1w_1 + x_2w_2 + b < 0$ (<a href=\"https://www.desmos.com/calculator/hsll1powpe\">see for yourself</a>, if that's not clear!).\n",
    "\n",
    "*So!* This is the understanding of logistic regression I've been building up to:\n",
    "\n",
    "> Logistic regression is almost identical to linear regression, in that it finds weights and biases that best \"fit\" the data. Unlike linear regression, which tries to find a line that best fits the general trend of the data, logistic regression finds a decision boundary that best separates the data into their correct classes. Data points are then compared to the decision boundary (via $x_1w_1 + x_2w_2 + ... + x_nw_n + b$), and converted to a probability of belonging to the positive class by passing the resulting value into the sigmoid function. That's it!\n",
    "\n",
    "Two quick things before running some code: how could we visualize a logistic regression model like we did linear regression? The only thing we would need to add is the sigmoid function between the inputs and outputs. There's no standard way of drawing this, but I'll illustrate it like so:\n",
    "\n",
    "<img src=\"images/log_reg.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7eeb0",
   "metadata": {},
   "source": [
    "Here, we can call the sigmoid function an **activation function**. You'll hear more about that when we get to deep learning :)\n",
    "\n",
    "Finally, which loss function is used in logistic regression? Typically, it's <a href=\"https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\">`cross-entropy loss`</a> (aka `log loss`). It's not necessary to go over that in detail, but it's worth reminding ourselves that models need some metric to optimize in order to \"learn\" the data.\n",
    "\n",
    "### Less math, more code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa60a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make classification task\n",
    "X_class, y_class = datasets.make_blobs(centers=2, random_state=95, cluster_std=3)\n",
    "# Print first 10 samples (each has two features)\n",
    "print(X_class[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 10 labels\n",
    "print(y_class[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "plt.scatter(X_class[:, 0], X_class[:, 1], marker='o', c=y_class, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9738865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Fit model to data\n",
    "classifier = LogisticRegression().fit(X_class, y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17861f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First ten predictions: {classifier.predict(X_class[:10])}\")\n",
    "print(f\"First ten actual labels: {y_class[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af0e75",
   "metadata": {},
   "source": [
    "We can also plot the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cdc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.coef_ gets us [[w_1, w_2]] and classfier.intercept_ gets us b\n",
    "# To get traditional slope and y-intercept, we need to rearrange ax + by = c into y = (-a/b)x - (c/b)\n",
    "decision_m = -classifier.coef_[0,0] / classifier.coef_[0,1]\n",
    "decision_b = classifier.intercept_ / classifier.coef_[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e173f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_class[:, 0], X_class[:, 1], marker='o', c=y_class, s=25, edgecolor='k')\n",
    "abline(decision_m, decision_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b124ee59",
   "metadata": {},
   "source": [
    "That worked well! But, just like linear regression, logistic regression has a weakness- it can only learn linear decision boundaries, like lines and planes. Here's some data that definitely isn't linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de59f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "X_gauss, y_gauss = datasets.make_gaussian_quantiles(n_features=2, \n",
    "                                 n_classes=2, \n",
    "                                 random_state=12)\n",
    "\n",
    "plt.scatter(X_gauss[:, 0], X_gauss[:, 1], marker='o', c=y_gauss, s=25, edgecolor='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6431a2",
   "metadata": {},
   "source": [
    "Good luck, linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04312b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "classifier_2 = LogisticRegression().fit(X_gauss, y_gauss)\n",
    "decision_m = -classifier.coef_[0,0] / classifier.coef_[0,1]\n",
    "decision_b = classifier.intercept_ / classifier.coef_[0,1]\n",
    "# Plot data again\n",
    "plt.scatter(X_gauss[:, 0], X_gauss[:, 1], marker='o', c=y_gauss, s=25, edgecolor='k')\n",
    "# Plot decision boundary\n",
    "abline(decision_m, decision_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c522a34",
   "metadata": {},
   "source": [
    "Again, it's literally the best logistic regression could do, but it's a bad fit. Not the right algorithm for this data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7ebce",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "Here's a summary of linear and logistic regression:\n",
    "\n",
    "model | linear regression | logistic regression\n",
    "- | - | -\n",
    "task | regression | classification\n",
    "linear aspect | finds line of best fit | finds linear decision boundary\n",
    "activation function | none / linear | sigmoid\n",
    "loss | SSE or MSE | cross-entropy / log loss\n",
    "\n",
    "In both cases, the model must learn a weight to assign to each input variable, as well as a bias. Then, the inputs are multiplied by their respective weights and summed together with the bias (i.e. $x_1w_1 + x_2w_2 + ... + x_nw_n + b$). The main mathematical difference between linear and logistic regression is that logistic regression then passes that value into the sigmoid function to output a probability between 0 and 1. The model makes predictions, and the parameters (weights and bias) are updated to minimize the loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1be3c",
   "metadata": {},
   "source": [
    "## Break\n",
    "<img src=\"images/brain.jpg\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d4422",
   "metadata": {},
   "source": [
    "## 4. Case Study\n",
    "Now that we've dealt with the theory, let's see what linear and logistic regression are like in practice!\n",
    "\n",
    "We'll be dealing with real estate data, so let's go ahead and import the data into the variable `house_data`. Don't worry about all of the `pandas` (`pd`) stuff, the comments will explain what each step is doing; the main focus is on the machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv (comma separated values) file into a pandas DataFrame\n",
    "# A DataFrame is basically a table\n",
    "house_data = pd.read_csv(\"data/houses.csv\")\n",
    "house_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd3a88b",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of houses (1460), each of which has 81 features, like the total lot area and the price it sold for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e9a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all columns\n",
    "house_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8d1cdf",
   "metadata": {},
   "source": [
    "You can see exactly what each variable means in `data/houses.txt`.\n",
    "\n",
    "### Regression\n",
    "\n",
    "Let's try to predict sale price from the overall quality (`1-10`) and lot area. First, we should visualize our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567312dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=house_data[\"OverallQual\"], y=house_data[\"SalePrice\"])\n",
    "plt.title(\"Sale price vs. overall quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcde48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=house_data[\"LotArea\"], y=house_data[\"SalePrice\"])\n",
    "plt.title(\"Sale price vs. lot area\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc975f7",
   "metadata": {},
   "source": [
    "We can see there's a bunch of outliers; properties with *huge* lot areas. Since there aren't that many of them, let's just delete them. They'll hurt the regression more than they'll help, and we probably wouldn't be super interested in these extreme cases anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2484e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out large lot areas\n",
    "house_data = house_data[house_data[\"LotArea\"] < 50000]\n",
    "# Plot again\n",
    "sns.scatterplot(x=house_data[\"LotArea\"], y=house_data[\"SalePrice\"])\n",
    "plt.title(\"Sale price vs. lot area\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204ff55",
   "metadata": {},
   "source": [
    "Better! For completeness, let's plot lot area against overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ce0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=house_data[\"OverallQual\"], y=house_data[\"LotArea\"])\n",
    "plt.title(\"Lot area vs overall quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd9a70",
   "metadata": {},
   "source": [
    "Both variables seem to have a positive association with the sales price, although there's a few asterisks. `SalePrice vs LotArea` has a **lot** of variance, and overall quality is a discrete quantity, so we should keep in mind not to use non-integer qualities or qualities outside of the range 1 to 10. There's no glaring red flags, though, so we can probably continue. What's also interesting is that lot area and overall quality don't seem to be particularly associated, which is a good thing- that means they each bring unique information that can help with the regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c040e62",
   "metadata": {},
   "source": [
    "Before we pull out the good old `.fit()`, let's split up our data into a **training** and **test** set. We'll use the training set to train the model, and test set to evaluate it. We'll go over why this is so important next session, but hopefully it makes sense why we'd want to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10331bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_data = house_data[[\"LotArea\", \"OverallQual\"]]\n",
    "y_data = house_data[\"SalePrice\"]\n",
    "# Separate X and y data into training and test sets; use 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b7273",
   "metadata": {},
   "source": [
    "Now we can fit our linear regression model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "# Fit model\n",
    "model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ff5f43",
   "metadata": {},
   "source": [
    "To see how well it did, we can check the $R^2$. $R^2$ basically represents the percent of variance in the output variable that's explained by our regression line. $R^2=0$ means that our model explains literally none of the variance in the output variable, and $R^2=1$ means our model explains all of it (i.e. perfectly predicts every target). As a rule of thumb, $R^2 > 0.75$ is considered \"pretty good\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 on training set\n",
    "model.score(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5c9ac",
   "metadata": {},
   "source": [
    "The training score doesn't really tell us much though, we want to check its performance on data it *hasn't* seen before: the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf796199",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed601e",
   "metadata": {},
   "source": [
    "Alright! It's not a fantastic score, but it's not a terrible one either. It looks like we can roughly predict the price of a house given its lot area and overall quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c50ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lot_size = 10000\n",
    "quality = 5\n",
    "\n",
    "print(f\"Lot area (sq. ft.): {lot_size}\")\n",
    "print(f\"Overall quality (1-10): {quality}\")\n",
    "\n",
    "# Order matters! Needs to match what it learned from X_train\n",
    "pred = model.predict([[lot_size, quality]])\n",
    "print(f\"The model predicts that this house will sell for ${round(pred[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703e6cc",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "What if we wanted to predict whether a given house had a fireplace or not? To make things more challenging, let's not use the sale price as a predictor. Instead, we'll use the first floor square footage, second floor square footage, and the wooden deck square footage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = house_data[[\"1stFlrSF\", \"2ndFlrSF\", \"WoodDeckSF\"]]\n",
    "y_data = house_data[\"Fireplaces\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab64163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the frequency of the y values\n",
    "y_data.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97f4f0",
   "metadata": {},
   "source": [
    "We have the number of fireplaces, but we just want 0 (no fireplace) or 1 (fireplace). So, let's transform the data a bit..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64090015",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y_data.apply(lambda x: 1 if x > 0 else 0)\n",
    "y_data.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4aa9d",
   "metadata": {},
   "source": [
    "Great! Now we can split our data and fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=41)\n",
    "classifier = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e56b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_floor = 1000\n",
    "second_floor = 0\n",
    "deck = 100\n",
    "\n",
    "print(f\"First floor area (sq. ft.): {first_floor}\")\n",
    "print(f\"Second floor area (sq. ft.): {second_floor}\")\n",
    "print(f\"Wooden deck area (sq. ft.): {deck}\")\n",
    "\n",
    "pred = classifier.predict([[first_floor, second_floor, deck]])\n",
    "print(f\"The model predicts that this property {'does' if pred[0] else 'does not'} have a fireplace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f14be01",
   "metadata": {},
   "source": [
    "There are a lot of options for evaluating a classifier. We'll cover this more in a later session to not get distracted, but for now let's just check accuracy (percent of all predictions that are correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "classifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1eb195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942a8c6",
   "metadata": {},
   "source": [
    "Again, not bad! 75% is an okay accuracy, especially considering we randomly chose our input data, used only two features, and did no data processing. If we had more time to spend, we would plot different distributions against each other (e.g. first floor square footage distribution for fireplace vs. no fireplace properties) and determine which variables would be most predictive, but we've already done a lot today and data visualization could be another workshop in and of itself!\n",
    "\n",
    "### Case Study Takeaways\n",
    "\n",
    "- Spend a lot more time than we did here exploring and visualizing your data and seeing how they relate to your target variable\n",
    "- Split your data into training and test data; training on your training data and evaluate on your test data\n",
    "- Always keep in mind that your model choice matters. So far we've only met one model for regression and one model for classification, but in the future you'll have more options to choose from!\n",
    "- For evaluating regression, $R^2$ is often a good choice. For classification, accuracy is one option (but can be misleading). Stay tuned for more options!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
