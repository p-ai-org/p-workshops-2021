{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac59a70",
   "metadata": {},
   "source": [
    "# P-ai AI/ML Workshop: Session 3\n",
    "\n",
    "Welcome to P-ai's third session of the AI/ML workshop series! Today we'll learn about\n",
    "- Gradient descent\n",
    "- A host of new machine learning algorithms to try out\n",
    "- How to use Git and GitHub\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/5d5aca05ce74150001a5af3e/1580018583262-NKE94RECI46GRULKS152/Screen+Shot+2019-12-05+at+11.18.53+AM.png?content-type=image%2Fpng\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266dfde",
   "metadata": {},
   "source": [
    "### Python warm-up\n",
    "\n",
    "Write a function that takes in a list `arr` and a search value `v`. The function returns the first value in `arr` that is closest to `v`. For example, `findClosest([1, 4, 7, 1], 2) -> 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c283df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosest(arr, v):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08409f05",
   "metadata": {},
   "source": [
    "## 0. Session 2 Review\n",
    "\n",
    "Here are some key points from last week's session:\n",
    "- Supervised learning, unsupervised learning, reinforcement learning\n",
    "    - Supervised: have labeled data, want model to predict $y$ from $X$ (e.g. predict house price from picture)\n",
    "    - Unsupervised: don't have labeled data, want to learn patterns directly in data (e.g. clustering)\n",
    "    - Reinforcement: want to train agent to perform actions in an environment that optimize some reward (e.g. play Mario)\n",
    "- Linear regression\n",
    "    - Used to predict a value from ≥1 input variable(s)\n",
    "    - Expects linear relationship between input and output\n",
    "- Logistic regression\n",
    "    - Used to predict a binary class (0 or 1) from ≥1 input variable(s)\n",
    "    - Finds linear decision boundary to separate data from different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f77cab3",
   "metadata": {},
   "source": [
    "## 1. Gradient descent\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/28/Chocolate_Hills_overview.JPG/1200px-Chocolate_Hills_overview.JPG\" width=\"500px\">\n",
    "\n",
    "What's gradient descent, and why are we talking about it now? This is actually one answer to a question that was posed in the previous two workshops, but which we didn't have time to answer in detail:\n",
    "\n",
    "> *How* do machine learning models find the optimal parameters?\n",
    "\n",
    "There's an asterisk here, because not all machine learning algorithms use gradient descent, like decision trees (which we'll cover later today). That being said, it's pretty pervasive in machine learning and absolutely crucial in deep learning. Let's get started!\n",
    "\n",
    "Imagine you're blind, and you're standing on the side of a hill. You want to go downward- how do you know which way to go? You can feel with your feet *which direction the slope is*, and then go in the opposite direction of the upward slope. You then *walk some distance* in that direction, and *repeat the process*. That's pretty much it! Gradient descent is a fairly intuitive process.\n",
    "\n",
    "> So what do the hills represent?\n",
    "\n",
    "The hills represent **loss** (reminder: loss is a synonym for cost; the thing you want to minimize). If we're sticking with three dimensions for the sake of visualizing, that means that if you pick two coordinates (say, $x$ and $y$), there's a height (loss) associated with those coordinates. Also recall that in the case of 2D linear regression, we want to find a **weight** and a **bias**; let's use $w$ and $b$ as our variables. Also, we'll call the loss $J$ (it's just convention). That means that we want to find:\n",
    "$$\n",
    "J(w, b)_{min}\n",
    "$$\n",
    "**\\[WARNING\\] CALCULUS INCOMING**  \n",
    "(If you're not comfortable with calculus, don't worry about this next stuff– but if you want to get as concrete as possible, this is for you)\n",
    "\n",
    "Mathematically, slope can be found by taking the *derivative* of a function. When you take the partial derivative of a function in all directions, that's called the *gradient*. The gradient of a function is a *vector* that points in the **direction of steepest ascent**. So, to go *down the gradient*, we would ideally wish to find the gradient of the loss function with respect to the weights, then take the *negative of the gradient* (direction of steepest descent), and update the weights in that direction.\n",
    "\n",
    "Here's that same statement, written more in more math-y terms:\n",
    "$$\n",
    "w \\leftarrow w - \\alpha \\nabla J\n",
    "$$\n",
    "Where $\\leftarrow$ means \"set equal to\", $\\nabla$ is the symbol for taking the gradient, and alpha ($\\alpha$) is what's called the **learning rate**. The learning rate decides *how much* to update the weight based on the gradient. Let's try to visualize this a little better; here's an example where the loss ($J$) is a function of only one variable:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg\" width=\"400px\">\n",
    "\n",
    "You can see that the weight is inialized all the way at the left, and it computes the gradient (in one dimension, this is just the derivative times the $\\hat{w}$ unit vector), and goes in the opposite direction, scaled by some amount ($\\alpha$). Eventually, it reaches a local minimum.\n",
    "\n",
    "> Why doesn't the weight get updated once it reaches the local minimum?\n",
    "\n",
    "Because, at the local minimum, the gradient is 0 in all directions. That means that $\\alpha \\nabla J = 0$, so $w$ doesn't update.\n",
    "\n",
    "Let's take a peek at an example with two input variables:\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/TbZwC.png\" width=\"500px\">\n",
    "\n",
    "In this example, you can see that there are multiple local minima, and vanilla gradient descent stops once it finds one, even if it isn't the *global* minimum. Don't forget what the \"surface\" is: it's the *loss* as a function of the model parameters, and we want to find *which* parameters result in the *lowest* loss so the model can *learn* the data best.\n",
    "\n",
    "(Side note: there are ways of avoiding the \"stuck in a local minimum problem, like introducing *momentum*. A very common optimizer that uses momentum is called <a href=\"https://ruder.io/optimizing-gradient-descent/index.html#adam\">Adam</a>)\n",
    "\n",
    "Okay! Enough of that for now, let's summarize what all of this was about:\n",
    "\n",
    "- Loss is a function of the model parameters, like weights and biases\n",
    "    - That is, there are certain combinations of parameters that minimize loss $\\approx$ maximize \"learning\"\n",
    "- Gradient is like slope, but it has a direction (always points in the direction of steepest ascent)\n",
    "- To find a minimum (global, hopefully, but not necessarily), we \"descend\" the \"gradient\"\n",
    "- At each step, the model makes a prediction and uses the difference between the predicted and actual value to estimate the gradient\n",
    "- The weights are then updated accordingly to incrementally decrease the loss, and the process repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc910d",
   "metadata": {},
   "source": [
    "## 2. Some more (supervised learning) algorithms!\n",
    "\n",
    "Up until now, we've only covered linear regression and logistic regression. There's so many commonly used ML models for supervised learning, we definitely won't have enough time to cover all of them in detail, but we can do a quick crash course through some of the most common ones, and you should feel free to experiment with these on your own later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ff9cd",
   "metadata": {},
   "source": [
    "## 2.1. [K-Nearest Neighbors (KNN)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*8Kfk4T5bsyCVU-jkHZYByg.png\" width=\"500px\">\n",
    "\n",
    "K-Nearest Neighbors is potentially the most intuitive machine learning model in existence, and I'm almost sorry we didn't start with KNN instead of linear and logistic regression, but I guess that just means you're ahead of the curve now. Anyway, let's get a bit of intuition about KNN:\n",
    "\n",
    "### Intuition\n",
    "KNN is predicated on a very simple observation:\n",
    "\n",
    "> Birds of a feather flock together\n",
    "\n",
    "Or, as Mocky once sang (and Vulfpeck later covered):\n",
    "\n",
    "> [Birds of a feather rock together](https://www.youtube.com/watch?v=sUAb1W_z1SE)\n",
    "\n",
    "Or, as we might interprete that:\n",
    "\n",
    "> Data from the same class have similar features\n",
    "\n",
    "Let's say you want to predict whether an email is spam or not based on the number of typos and the word count. If you had a bunch of data and plotted it, you might see that the \"spam\" data points group together in the \"low word count and many typos\" corner. If I gave you a new email and placed it on the plot, you could pretty easily guess its legitimacy based on where it sits compared to the known spam vs. non-spam emails. If it lands right in the middle of the \"spam\" cluster, you'd probably label it as \"spam\", and vice versa. This is the whole idea behind KNN, just a little more formalized.\n",
    "\n",
    "> **Algorithm**: To determine which class a new data point belongs to, choose the k nearest neighbors, where k is a (typically odd) positive integer. The class of the new point is the class which most of the neighbors belong to.\n",
    "\n",
    "Note that $k=1$ is literally \"nearest neighbor\"; a new unlabeled data point just gets the class of the nearest data point. If you chose $k=3$, you would look at the three nearest neighbors, and they would \"vote\" on the class of the new data point.\n",
    "\n",
    "<img src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final1_ibdm8a.png\" width=\"500px\">\n",
    "\n",
    "### Pros and cons\n",
    "\n",
    "- <u>Pros</u>:\n",
    "    - Simple and intuitive\n",
    "    - No training necessary\n",
    "- <u>Cons</u>:\n",
    "    - As the dataset grows, time to find nearest neighbors slows down\n",
    "    - Curse of dimensionality! (more on this to come later)\n",
    "    - To use regular (Euclidean) distance, all input variables should have the same units\n",
    "    - Performs poorly on unbalanced data\n",
    "    \n",
    "### Example\n",
    "We'll use [sklearn's implementation of KNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1171f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edace206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_data(X_data, y_data, n_classes, labels=None):\n",
    "    classes = np.array([[row for j, row in enumerate(X_data) if y_data[j] == i] for i in range(n_classes)])\n",
    "    for i, class_i in enumerate(classes):\n",
    "        plt.scatter(class_i[:, 0], class_i[:, 1], label=f\"class {i}\")\n",
    "    plt.legend()\n",
    "    if labels is not None:\n",
    "        plt.xlabel(labels[0])\n",
    "        plt.ylabel(labels[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_guesses(X, y_pred, y_actual, labels=None):\n",
    "    correct = np.array([row for i, row in enumerate(X) if y_pred[i] == y_actual[i]])\n",
    "    incorrect = np.array([row for i, row in enumerate(X) if y_pred[i] != y_actual[i]])\n",
    "    if correct.shape[0]:\n",
    "        plt.scatter(correct[:, 0], correct[:, 1], c=\"g\", label=f\"correct\")\n",
    "    if incorrect.shape[0]:\n",
    "        plt.scatter(incorrect[:, 0], incorrect[:, 1], c=\"r\", label=f\"incorrect\")\n",
    "    plt.legend()\n",
    "    if labels is not None:\n",
    "        plt.xlabel(labels[0])\n",
    "        plt.ylabel(labels[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "# Create dataset\n",
    "X_gauss, y_gauss = datasets.make_gaussian_quantiles(n_features=2, \n",
    "                                n_classes=2, \n",
    "                                random_state=12,\n",
    "                                n_samples=100)\n",
    "plot_classification_data(X_gauss, y_gauss, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719dbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_gauss, y_gauss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a4517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with k=5\n",
    "k = 5\n",
    "neigh = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
    "neigh.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "samples_to_predict = [[-1, -2], [0.5, 0]]\n",
    "for sample_to_predict in samples_to_predict:\n",
    "    prediction = neigh.predict([sample_to_predict])\n",
    "    print(f\"The model predicted that the sample {sample_to_predict} belongs to class {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7356600",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neigh.predict(X_test)\n",
    "plot_guesses(X_test, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75de669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "neigh.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308bad0f",
   "metadata": {},
   "source": [
    "As you can see, the model can't be perfect because there's some overlap between the two classes. Still, this might be as well as we could expect any model to perform without overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427b1a4",
   "metadata": {},
   "source": [
    "## 2.2. [Naive Bayes (NB)](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)\n",
    "<img src=\"https://i.imgur.com/utT5nAr.jpg\" width=\"400px\">\n",
    "\n",
    "Naive Bayes is a simple, but often surprisingly powerful algorithm used for classification and regression. There are two main flavors, Multinomial Naive Bayes (MNB) and Gaussian Naive Bayes (GNB). The main difference is that MNB works best with discrete features (e.g. word counts) and GNB works best with roughly normal (i.e. from a bell curve) features. Naive Bayes is typically very fast and lightweight.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The cornerstone of Naive Bayes is what's called Bayes' Theorem, and it looks like this:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This basically says, *the probability of A given B is the probability of B given A, times the probability of A, divided by the probability of B.* As an example, let's assume the following:\n",
    "\n",
    "- 60% of college students like pizza\n",
    "- 15% of college students are majoring in CS\n",
    "- 80% of CS majors like pizza\n",
    "\n",
    "If you meet a college student who likes pizza, what is the probability that they are a CS major?\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P(CS|Pizza) &= \\frac{P(Pizza|CS)P(CS)}{P(Pizza)} \\\\\n",
    "P(CS|Pizza) &= \\frac{0.8 \\times 0.15}{0.6} \\\\\n",
    "P(CS|Pizza) &= 0.2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So there is a 20% chance that a given college student is a CS major given they like pizza. You can also expand upon this concept to allow for multiple input variables (i.e. $P(A|B_1 \\wedge B_2 \\wedge B_3 ...)$, or in words, \"the probability of $A$ given $B_1$ *and* $B_2$ *and* $B_3$ etc...).\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "<u>Pros</u>:\n",
    "- Tends to be resistant to noise (e.g. irrelevant features, outliers)\n",
    "- Since NB is extremely efficient, it works very well on large datasets\n",
    "- Scales linearly (size of model grows with size of the dataset), which is realistically ideal\n",
    "- Works well with high-dimensional data\n",
    "\n",
    "<u>Cons</u>:\n",
    "- Must be used on non-correlated input features (more on this below)\n",
    "- Often requires a good amount of data for the statistics to work out\n",
    "\n",
    "The main con has to do with why Naive Bayes is called \"naive\": it makes a very strong assumption about the input data. Namely, it assumes that **all input features are independent of each other**. In \"real-world\" applications, this is pretty rare. For example, you theoretically wouldn't be able to include `age` and `weight` as features because age and weight are correlated. Still, ML engineers like ignoring statistical conditions, so there's no rule against using NB anyway, just know that it may affect the model's performance.\n",
    "\n",
    "<img src=\"images/nb-meme.png\" width=\"400px\">\n",
    "\n",
    "### Example\n",
    "\n",
    "We'll use the [sklearn implementation of Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c053c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_data(X_gauss, y_gauss, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda639aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Create GNB classifier\n",
    "clf = GaussianNB()\n",
    "# Fit classifier to training data- remember there are many parameters you can pass in, check the documentation!\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "samples_to_predict = [[-1, -2], [0.5, 0]]\n",
    "for sample_to_predict in samples_to_predict:\n",
    "    prediction = clf.predict([sample_to_predict])\n",
    "    print(f\"The model predicted that the sample {sample_to_predict} belongs to class {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad6862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "plot_guesses(X_test, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928bbc0e",
   "metadata": {},
   "source": [
    "## 2.3. [Decision Tree](https://scikit-learn.org/stable/modules/tree.html) & [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "### Intuition\n",
    "Decision trees are another example of intuitive machine learning models. To classify an example, the algorithm asks a series of \"questions\" about it, and assigns it a label based on the \"answers\" to those questions. Take a look at the following decision tree for classifying fruit:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/698/1*zDqdNSWEqcRym1D8SLAmhw.jpeg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea53a506",
   "metadata": {},
   "source": [
    "So in other words, a decision tree is basically a flow chart. However, in a vanilla decision tree, the \"questions\" must all be binary (e.g. `fruit_length > 2` can only be `True` or `False`).\n",
    "\n",
    "But how does the model learn which questions to ask? Basically, during training, the model follows an algorithm to determine which questions will be the most \"decisive\"; that is, will best separate one class from another. To gain some intuition, let's imagine two games of 20 Questions:\n",
    "\n",
    "**Player 1**  \n",
    "Q1. Is it alive? `Yes`  \n",
    "Q2. Is it composed of cells? `Yes`  \n",
    "Q3. Does it live on Earth? `Yes`  \n",
    "...\n",
    "\n",
    "**Player 2**  \n",
    "Q1. Is it alive? `Yes`  \n",
    "Q2. Does it live underwater? `Yes`  \n",
    "Q3. Is it a mammal? `Yes`  \n",
    "...\n",
    "\n",
    "It should be obvious that Player 2 is the superior guesser, but why? Player 2 chooses questions whose answers will roughly cut in half the number of possible \"things\" the answer can be. The perfect question is one whose answer completely determines what the answer is. For example:\n",
    "\n",
    "Q1. Is it alive? `Yes`  \n",
    "Q2. Does it live underwater? `Yes`  \n",
    "Q3. Is it a mammal? `Yes`  \n",
    "Q4. Can it also live on land? `No`  \n",
    "Q5. **Are they the largest mammals on Earth?** `Yes => whale; No => dolphin`\n",
    "\n",
    "Player 1, on the other hand, asked questions that added no new information, since it doesn't split the possible space of \"things\" at all.\n",
    "\n",
    "Decision trees train in a similar way, typically using something called the [gini impurity](https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-gini-impurity/). Gini impurity is a measure of how well the question separates examples of different classes in the dataset.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1200/1*z7tK94rGGIy_42UpiqilLQ.png\" width=\"500px\">\n",
    "\n",
    "If you were trying to predict whether passengers aboard the Titanic survived or died, you can imagine that a question like `\"Did their last name start with an A?\"` would be a poor choice, since the first letter of their last name likely has little to do with their survival, and so it would split the passengers more or less randomly (the first picture above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d460bef",
   "metadata": {},
   "source": [
    "### Pros and cons\n",
    "<u>Pros</u>:\n",
    "- Intuitive\n",
    "- Explainable\n",
    "- Usually doesn't require normalization\n",
    "\n",
    "<u>Cons</u>:\n",
    "- Tends to overfit (unstable)\n",
    "- Small change in data can greatly affect tree structure (unstable)\n",
    "- Doesn't work well on regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e2af8",
   "metadata": {},
   "source": [
    "### Example\n",
    "Good old scikit-learn; we'll be using [their implementation of the decision tree algorithm](https://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272911f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e861b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_data(X_gauss, y_gauss, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d740c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f6428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction\n",
    "samples_to_predict = [[-1, -2], [0.5, 0]]\n",
    "for sample_to_predict in samples_to_predict:\n",
    "    prediction = clf.predict([sample_to_predict])\n",
    "    print(f\"The model predicted that the sample {sample_to_predict} belongs to class {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb337b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "plot_guesses(X_test, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8334343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f6f086",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Random forest is an example of **ensemble learning**; a model that uses multiple different models to contribute to a single answer. In particular, random forest is a collection of independent decision trees that each \"cast a vote\" as to what the classification is, and the random forest chooses the label with the most \"votes\". This generally helps prevent some of the instability issues of a single decision tree, at the expense of more computation.\n",
    "\n",
    "<img src=\"https://www.tibco.com/sites/tibco/files/media_entity/2021-05/random-forest-diagram.svg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bddcac",
   "metadata": {},
   "source": [
    "As usual, let's try an example with [sklearn's implementation of the random forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=3)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "plot_guesses(X_test, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832131b",
   "metadata": {},
   "source": [
    "As expected, the random forest model outperforms a single decision tree. Let's move on to our last algorithm for today!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa76ffa",
   "metadata": {},
   "source": [
    "## 2.4. [Support Vector Machine (SVM)](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n",
    "\n",
    "Support Vector Machines include Support Vector Classification (SVC) and Support Vector Regression (SVR), so you can actually use this model for both classification and regression tasks! We'll focus on SVC because it illustrates the main concept of the \"support vector\" better, but don't forget that SVMs to regression, too.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Let's say you had never heard of apples or lemons, and you wanted to learn to distinguish the two. The general descriptions of the two fruits might be `\"apples are red and round\"` and `\"lemons are yellow and oval\"`. But, you might run into trouble with a yellow apple or a round, unripe lemon. So, instead of learning the general characteristics of each class, you might focus on the most \"confusing\" examples from each class, and use them to form a decision boundary. Those \"confusing\" examples are called **support vectors**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/0*eH4sQUkJc8YT76jL.png\" width=\"500px\">\n",
    "\n",
    "*To see a more in-depth analysis of SVM using this same analogy, check out [this Medium article](https://towardsdatascience.com/svm-and-kernel-svm-fed02bef1200).*\n",
    "\n",
    "Using the support vectors, SVM finds the **optimal hyperplane** (read: best decision boundary) to separate the training data.\n",
    "\n",
    "### Kernel trick\n",
    "\n",
    "Like logistic regression, vanilla SVMs can only find linear decision boundaries. That being said, SVMs implement something called the \"kernel trick\" that can sometimes turn non-linearly separable data into linearly separable data! Here's some visual intuition for how it can work:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*mCwnu5kXot6buL7jeIafqQ.png\" width=\"500px\">\n",
    "\n",
    "By applying a **kernel function** to our data (in this case, a Gaussian kernel function), we can map our original data into a higher dimensional space where our classes are linearly separable. Pretty cool, right?\n",
    "\n",
    "### Pros and cons\n",
    "\n",
    "<u>Pros</u>:  \n",
    "- Effective in higher dimensions\n",
    "- Fast at prediction\n",
    "- Offers classification and regression\n",
    "\n",
    "<u>Cons</u>:  \n",
    "- Expensive (computationally) to train\n",
    "- Results aren't very interpretable / explainable\n",
    "- Don't scale very well to large datasets\n",
    "\n",
    "### Example\n",
    "\n",
    "You already know it; [sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_data(X_gauss, y_gauss, n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bb6b5",
   "metadata": {},
   "source": [
    "We'll use the RBF (Radial Basis Function) kernel; this is a pretty good default kernel to use. You can read more about it [here](https://towardsdatascience.com/radial-basis-function-rbf-kernel-the-go-to-kernel-acf0d22c798a)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744ec0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf')\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f51491",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "plot_guesses(X_test, y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e792e",
   "metadata": {},
   "source": [
    "Wow, 100% test accuracy! Still, let's calm down a bit– this is a pretty small test set, and the dataset isn't that difficult. That being said, SVMs can be surprisingly powerful, and in many cases you might get better performance with one of these than a neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4236e545",
   "metadata": {},
   "source": [
    "## 2.5. ML algorithms review\n",
    "\n",
    "We covered a lot of ground, so let's quickly wrap up the main idea for each ML algorithm:\n",
    "\n",
    "Model name                       | Description (to classify an example, ...)\n",
    "---------------------------------|------------\n",
    "K Nearest Neighbors (KNN)        | Choose the majority class from its `k` nearest neighbors\n",
    "Naive Bayes                      | Use Bayesian statistics\n",
    "Decision Tree (DT)               | Use a tree of boolean criteria\n",
    "Random Forest (RF)               | Use an ensemble of DTs to \"vote\" on the label\n",
    "Support Vector Machine (SVM)     | Find the optimal hyperplane by identifying support vectors and applying a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482062fa",
   "metadata": {},
   "source": [
    "## 3. Case study\n",
    "\n",
    "<img src=\"https://www.almanac.com/sites/default/files/image_nodes/iris-flowers.jpg\" width=\"500px\">\n",
    "\n",
    "Let's try applying one (or a few) of these machine learning models on a \"real world\" problem. The dataset we'll be using is the [Iris Flower Dataset](https://www.kaggle.com/arshid/iris-flower-dataset/version/1), which is the \"hello world\" of machine learning. Let's take a look at this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b989abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94903f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all values for species\n",
    "iris_data['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030a4a6",
   "metadata": {},
   "source": [
    "As we can see, we have four features (`sepal_length`, `sepal_width`, `petal_length`, `petal_width`), and a species (one of `Iris-setosa`, `Iris-versicolor`, and `Iris-virginica`). Maybe we can try to predict which species a flower belongs to based on their petal / sepal features!\n",
    "\n",
    "First, we should plot our data to see what we're getting ourselves into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffe833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names as a List\n",
    "columns = iris_data.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first four columns (X data)\n",
    "X_data = iris_data[columns[:4]]\n",
    "X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d14a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map from string species name to integer\n",
    "mapping = {\n",
    "    'Iris-setosa': 0,\n",
    "    'Iris-versicolor': 1,\n",
    "    'Iris-virginica': 2\n",
    "}\n",
    "\n",
    "# Apply mapping to new column species_int\n",
    "iris_data['species_int'] = iris_data['species'].transform(lambda x: mapping[x])\n",
    "iris_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = iris_data['species_int']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc38be4",
   "metadata": {},
   "source": [
    "Okay, now we're ready to plot the data. Notice that we have four input variables, but we can only show 2 variables on a 2D graph. In theory, there are 6 possible plots, but for convenience, we'll just plot `sepal_length` vs. `sepal_width`, and `petal_length` vs. `petal_width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265774c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data (as pandas Dataframes) into numpy arrays to get rid of the \"extra stuff\"\n",
    "for (start_index, end_index) in ((0, 2), (2, 4)):\n",
    "    plot_classification_data(\n",
    "        X_data=np.array(X_data)[:,start_index:end_index],\n",
    "        y_data=np.array(y_data), \n",
    "        n_classes=3, \n",
    "        labels=columns[start_index:end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386b508d",
   "metadata": {},
   "source": [
    "Alright! Right off the bat, we can see that class 0 (setosa) should be pretty easy to classify, but classes 1 and 2 (versicolor and virginica) have more overlap. Looking at the two plots, we can also see that versicolor and verginica are more separable when comparing petal length / width than when comparing sepal length. This is good to know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9052a",
   "metadata": {},
   "source": [
    "So, which model should we use? Since there's definitely a spatial relationship here and all dimensions have the same units (`mm`), we might as well start with the simple KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc6e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbitrarily choose k = 3\n",
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5475e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f9db2",
   "metadata": {},
   "source": [
    "So, our KNN classifier got almost 95% of the test dataset correct; that's pretty good! Let's take a look at where it tripped up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd74414",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = knn_classifier.predict(X_test)\n",
    "for (start_index, end_index) in ((0, 2), (2, 4)):\n",
    "    plot_guesses(\n",
    "        np.array(X_test)[:,start_index:end_index], \n",
    "        np.array(y_pred), \n",
    "        np.array(y_test),\n",
    "        labels=columns[start_index:end_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed65f5f8",
   "metadata": {},
   "source": [
    "While this plot is rather nice, it's hard to tell which species were misclassified. For this, we should use a **confusion matrix**. Here's an example of a confusion matrix:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*b5FxmrSTdcjnre99EdoNBQ.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cb028",
   "metadata": {},
   "source": [
    "A confusion matrix tells us what the **true vs. actual** label of different examples were, so that we can see which labels the model misclassifies, and **how**. In the example above, the model appears to be learning to classify cats, dogs, and fish. The sample size is very small, but we can draw some insights about how the model is classifying. Here are a few examples:\n",
    "- Every time the animal was a dog, the model correctly predicted a dog\n",
    "- Most (86%) of the time the model predicted a dog, it was correct\n",
    "- Most (88%) of the time the animal was actually a fish, the model correctly predicted a fish\n",
    "- On the other hand, when the animal was a cat, the model was wrong 66% of the time because it mistook the cat for a fish\n",
    "- Also, when the model predicted fish, 36% of the time, it was actually a cat\n",
    "\n",
    "From this, we can see that the model is mostly confusing cats and fish; and that it tends to overclassify animals as fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b42c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12fadf",
   "metadata": {},
   "source": [
    "That's our confusion matrix! Let's make it a bit prettier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn is built on top of matplotlib, more user-friendly (and generally better-looking)\n",
    "!pip install seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_show_cm(y_actual, y_pred):\n",
    "    cm = confusion_matrix(y_actual, y_pred)\n",
    "    sns.heatmap(cm, annot=True)\n",
    "    plt.xlabel('predicted')\n",
    "    plt.ylabel('actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705483d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_show_cm(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394da2b",
   "metadata": {},
   "source": [
    "We now can clearly see that the model misclassified two virginicas as versicolor. What if we increase `k` to 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier_2 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier_2.fit(X_train, y_train)\n",
    "y_pred_2 = knn_classifier_2.predict(X_test)\n",
    "compute_and_show_cm(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521769f8",
   "metadata": {},
   "source": [
    "Now we're down to one misclassification– this isn't really a meaningful difference, but it's worth pointing out that we improved the performance of our model by trying a different **hyperparameter**. A hyperparameter is a parameter of our model that the model **doesn't learn through training**. The number of neighbors `k` in a KNN is something that we choose; the model doesn't learn. That's why, very often in machine learning, the next step after training a baseline model is tuning hyperparameters.\n",
    "\n",
    "Just for the fun of it, let's try another model. What about SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel='rbf')\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svc_classifier.predict(X_test)\n",
    "compute_and_show_cm(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a6a87",
   "metadata": {},
   "source": [
    "So in this case, the SVM performs as well as a KNN with `k=5`. It's worth noting that our sample size is pretty small, so even if our SVM got 100% test accuracy, that might not be a meaningful difference over the KNN.\n",
    "\n",
    "That's it for the case study– let me know if you have any questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009428b4",
   "metadata": {},
   "source": [
    "## Git and GitHub\n",
    "\n",
    "I created a Notion document that outlines all the essentials about git, and you can find it [here](https://www.notion.so/P-ai-Guide-to-Git-143ee9bedf7e4959ba7601e7f2861e7c)! If you follow the link, you'll find a video of me going through the git / GitHub workflow, which I'm sure many of you will prefer over a written document. If you're reading this right now in one of the in-person workshops, I'll be doing this demo now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d93fa1",
   "metadata": {},
   "source": [
    "# See you next week!\n",
    "<img src=\"https://en.meming.world/images/en/thumb/0/04/Adios_Wormhole.jpg/300px-Adios_Wormhole.jpg\" width=\"500px\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
